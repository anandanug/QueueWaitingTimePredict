# -*- coding: utf-8 -*-
"""QueueWaitingTimePredict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SRdjtnOQ15bSQcj58yiVYA6EVKElx-7q

# Business Understanding

A company like retail stores, restaurants, or hospitals want to predict queue waiting time. So its can improve satisfaction customer and increase efficiency

# Data Understanding

* arrival_time --> time when customer arrival
* start_time --> time when customer start
* finish_time --> time when customer finish
* wait_time --> time when customer waiting
* queue_length --> number of queue customer

## Download Dataset
"""

# install library

!pip install kaggle

# create folder 

!mkdir ~/.kaggle

# copy kaggle.json to kaggle

!cp kaggle.json ~/.kaggle

# grant access

!chmod 600 ~/.kaggle/kaggle.json

# download dataset

!kaggle datasets download -d sanjeebtiwary/queue-waiting-time-prediction

# unzip dataset

!unzip /content/queue-waiting-time-prediction.zip

"""# Data Preparation

## Exploration Data Analysis
"""

# import library

import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
from datetime import timezone

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# load dataset

df = pd.read_csv("/content/queue_data.csv")

# display data head

df.head()

# check data info

df.info()

# check data describe

df.describe()

df.describe()

"""## Type Conversion"""

# check data type

df.dtypes

df.head()

# change data type

df['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%d-%m-%Y %H.%M').astype(int) // 10**9
df['start_time'] = pd.to_datetime(df['start_time'], format='%d-%m-%Y %H.%M').astype(int) // 10**9
df['finish_time'] = pd.to_datetime(df['finish_time'], format='%Y-%m-%d %H:%M:%S').astype(int) // 10**9

df.head()

"""## Remove Duplicates"""

# check duplicates

num_duplicates = df.duplicated().sum()

# print number of duplicates

print("Number of duplicates : ", num_duplicates)
print("Number rows before remove duplicates : ", len(df))

# drop duplicates

df.drop_duplicates(inplace=True)

# print
print("Number rows after remove duplicates : ", len(df))

"""## Handle missing values"""

# check missing values

df.isna().sum()

"""## Handle Outlier"""

# analysis outlier using visualization

sns.pairplot(df, kind='scatter')

# scaler = StandardScaler()
# X = scaler.fit_transform(X)

# scaler = MinMaxScaler()
# X = scaler.fit_transform(X)

"""# Modeling"""

# sns.heatmap(df.corr(), annot=True)

X = df[['arrival_time','start_time', 'finish_time']]
y = df['wait_time']

random_states = [0, 42, 123, 456, 789]

for random_state in random_states:
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state)

  # create model

  model = LinearRegression()

  # set hyperparameter to tune

  param_grid = {'copy_X' : [True, False],
                'fit_intercept' : [True, False],
                'n_jobs' : [-1,1],
                'positive' : [True, False]}

  # perform grid search

  grid_search = GridSearchCV(model, param_grid=param_grid, scoring='r2')
  grid_search.fit(X_train, y_train)

  # print best parameter

  print('Best Hyperparameter : ', grid_search.best_params_)

  # use the best hyperparameter to train

  lin_reg_best = LinearRegression(**grid_search.best_params_)
  lin_reg_best.fit(X_train, y_train)

  y_pred = lin_reg_best.predict(X_test)

  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)

  print("Pakai Data Test")
  print('Random State : ', random_state)
  print('MSE : ', mse)
  print("R-Squared : ", r2)
  print("\n")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 789)

# create model

model = LinearRegression()

# set hyperparameter to tune

param_grid = {'copy_X' : [True, False],
              'fit_intercept' : [True, False],
              'n_jobs' : [-1,1],
              'positive' : [True, False]}

# perform grid search

grid_search = GridSearchCV(model, param_grid=param_grid, scoring='r2')
grid_search.fit(X_train, y_train)

# print best parameter

print('Best Hyperparameter : ', grid_search.best_params_)

# use the best hyperparameter to train

lin_reg_best = LinearRegression(**grid_search.best_params_)
lin_reg_best.fit(X_train, y_train)

y_pred = lin_reg_best.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Pakai Data Test")
print('Random State : ', random_state)
print('MSE : ', mse)
print("R-Squared : ", r2)
print("\n")

# print(lin_reg_best.coef_)